# ğŸš€ Optimizaciones de Rendimiento - Socrates AI

## ğŸ“Š AnÃ¡lisis de Cuellos de Botella

### 1. **Carga Inicial de Conversaciones** (2-3 segundos)
- âŒ Se cargan TODOS los mensajes de una conversaciÃ³n sin lÃ­mite
- âŒ No hay paginaciÃ³n implementada
- âŒ Ordenamiento de mensajes en el backend despuÃ©s de traerlos

### 2. **Flujo de Chat API** (3-5 segundos)
- âŒ MÃºltiples operaciones sÃ­ncronas en serie
- âŒ InicializaciÃ³n de Zep en cada request
- âŒ Contexto cruzado que puede no ser necesario
- âŒ ActualizaciÃ³n del tÃ­tulo bloquea la respuesta

### 3. **Llamadas al LLM** (2-4 segundos)
- âŒ No hay streaming de respuestas
- âŒ Se envÃ­an hasta 10 mensajes previos (puede ser excesivo)
- âŒ Modelo pesado para todas las consultas
- âŒ No hay cachÃ© de respuestas comunes

## âœ… Optimizaciones Recomendadas

### **PRIORIDAD ALTA** ğŸ”´

#### 1. **Implementar PaginaciÃ³n de Mensajes**
```typescript
// apps/backend/src/app/api/conversations/[id]/route.ts
const { data: conversation, error } = await supabase
  .from('conversations')
  .select(`
    id,
    title,
    created_at,
    updated_at,
    messages (
      id,
      content,
      role,
      created_at
    )
  `)
  .eq('id', conversationId)
  .eq('user_id', userId)
  .order('messages.created_at', { ascending: false })
  .limit(20) // Solo cargar Ãºltimos 20 mensajes
  .single();
```

#### 2. **Implementar Streaming de Respuestas**
```typescript
// apps/backend/src/app/api/chat/route.ts
import { StreamingTextResponse, LangChainStream } from 'ai';

export async function POST(request: NextRequest) {
  const { stream, handlers } = LangChainStream();

  const model = new ChatGoogleGenerativeAI({
    modelName: 'gemini-1.5-flash',
    streaming: true,
    callbacks: [handlers],
  });

  // Generar respuesta con streaming
  model.invoke(chatMessages);

  return new StreamingTextResponse(stream);
}
```

#### 3. **OptimizaciÃ³n de Contexto**
```typescript
// Reducir contexto a solo 5 mensajes mÃ¡s relevantes
const recentMessages = messages?.slice(-5) || [];

// Solo obtener contexto cruzado si es necesario
const needsCrossContext = message.toLowerCase().includes('recuerdas') ||
                          message.toLowerCase().includes('antes');
```

### **PRIORIDAD MEDIA** ğŸŸ¡

#### 4. **Cache de Zep Sessions**
```typescript
// apps/backend/src/lib/zep.ts
const sessionCache = new Map<string, any>();
const CACHE_TTL = 5 * 60 * 1000; // 5 minutos

export async function getOrCreateSession(
  conversationId: string,
  userId: string
): Promise<any> {
  const cacheKey = `conversation_${conversationId}`;
  const cached = sessionCache.get(cacheKey);

  if (cached && Date.now() - cached.timestamp < CACHE_TTL) {
    return cached.session;
  }

  // Crear o obtener sesiÃ³n...
  sessionCache.set(cacheKey, { session, timestamp: Date.now() });
  return session;
}
```

#### 5. **ParalelizaciÃ³n de Operaciones**
```typescript
// apps/backend/src/app/api/chat/route.ts
// Ejecutar operaciones en paralelo
const [zepSession, userMessage] = await Promise.all([
  getOrCreateSession(conversationId, userId),
  supabase.from('messages').insert({
    conversation_id: conversationId,
    content: message,
    role: 'user',
  }).select().single()
]);
```

#### 6. **ActualizaciÃ³n AsÃ­ncrona del TÃ­tulo**
```typescript
// No bloquear la respuesta esperando actualizaciÃ³n del tÃ­tulo
if (isFirstMessage) {
  // Actualizar tÃ­tulo en background
  setImmediate(async () => {
    await supabase
      .from('conversations')
      .update({ title: truncatedMessage })
      .eq('id', conversationId);
  });
}
```

### **PRIORIDAD BAJA** ğŸŸ¢

#### 7. **Optimistic UI Updates**
```typescript
// apps/mobile/src/screens/ChatScreen.tsx
const sendMessage = async () => {
  // Mostrar mensaje del usuario inmediatamente
  const userMessage = {
    id: Date.now().toString(),
    content: inputText.trim(),
    role: 'user',
    createdAt: new Date().toISOString(),
  };

  setMessages(prev => [...prev, userMessage]);
  setInputText('');

  // Agregar mensaje "escribiendo..." temporal
  const tempAIMessage = {
    id: 'temp',
    content: '...',
    role: 'assistant',
    createdAt: new Date().toISOString(),
  };

  setMessages(prev => [...prev, tempAIMessage]);

  // Hacer llamada al API...
};
```

#### 8. **CachÃ© de Respuestas Comunes**
```typescript
// Cache para preguntas frecuentes
const commonResponses = new Map([
  ['hola', 'respuesta_predefinida'],
  ['ayuda', 'respuesta_ayuda'],
]);

if (commonResponses.has(message.toLowerCase())) {
  return NextResponse.json({
    message: commonResponses.get(message.toLowerCase()),
    cached: true,
  });
}
```

#### 9. **Lazy Loading de Mensajes Antiguos**
```typescript
// apps/mobile/src/screens/ChatScreen.tsx
const loadMoreMessages = async () => {
  if (!hasMore || loadingMore) return;

  setLoadingMore(true);
  const olderMessages = await fetchMessages(lastMessageId, 20);
  setMessages(prev => [...olderMessages, ...prev]);
  setLoadingMore(false);
};

// En FlatList
<FlatList
  onEndReached={loadMoreMessages}
  onEndReachedThreshold={0.5}
/>
```

## ğŸ“ˆ Resultados Esperados

### Tiempo de Carga Actual vs Optimizado:
- **Carga inicial**: 2-3s â†’ **0.5-1s** âœ…
- **EnvÃ­o de mensaje**: 3-5s â†’ **1-2s** âœ…
- **Respuesta del LLM**: 2-4s â†’ **0.5s (primer token)** âœ…

### Mejoras de UX:
- âœ… Feedback instantÃ¡neo al usuario
- âœ… Streaming de respuestas (aparecen palabra por palabra)
- âœ… Menor consumo de datos mÃ³viles
- âœ… Menor uso de memoria

## ğŸ› ï¸ Plan de ImplementaciÃ³n

### Fase 1 (Inmediato):
1. Implementar paginaciÃ³n de mensajes
2. Reducir contexto del LLM
3. Paralelizar operaciones

### Fase 2 (Esta semana):
1. Implementar streaming de respuestas
2. Agregar cache de sesiones
3. Optimistic UI updates

### Fase 3 (PrÃ³xima semana):
1. Lazy loading de mensajes
2. Cache de respuestas comunes
3. MÃ©tricas de rendimiento

## ğŸ“Š MÃ©tricas a Monitorear

```typescript
// Agregar logging de mÃ©tricas
const startTime = Date.now();

// ... operaciÃ³n ...

console.log({
  operation: 'chat_response',
  duration: Date.now() - startTime,
  conversationId,
  messageLength: message.length,
  contextSize: messages.length,
});
```

## ğŸ”§ ConfiguraciÃ³n Recomendada

### Variables de Entorno Nuevas:
```env
# OptimizaciÃ³n
MAX_CONTEXT_MESSAGES=5
ENABLE_RESPONSE_STREAMING=true
CACHE_TTL_SECONDS=300
ENABLE_COMMON_CACHE=true
```

### Modelo LLM Optimizado:
```typescript
// Para respuestas rÃ¡pidas
const fastModel = new ChatGoogleGenerativeAI({
  modelName: 'gemini-1.5-flash', // MÃ¡s rÃ¡pido que flash-exp
  temperature: 0.7,
  maxOutputTokens: 512, // Reducir para respuestas mÃ¡s rÃ¡pidas
});

// Para respuestas complejas (matemÃ¡ticas, etc)
const complexModel = new ChatGoogleGenerativeAI({
  modelName: 'gemini-2.0-flash-exp',
  temperature: 0.7,
  maxOutputTokens: 1024,
});
```

## ğŸ¯ ConclusiÃ³n

Con estas optimizaciones, podemos reducir el tiempo de respuesta total de **5-8 segundos** a **1-2 segundos**, mejorando significativamente la experiencia del usuario.

Las optimizaciones de **PRIORIDAD ALTA** deberÃ­an implementarse primero ya que tienen el mayor impacto con el menor esfuerzo.